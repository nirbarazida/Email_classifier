{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving your model exercise. Part 2 - Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder\n",
    "Reminder, there will usually be 3 different places where the code relevant to our model prediction runs:\n",
    "1. **Training computer / server** - where we train our model and save it\n",
    "2. **Inference server** - server that listens to REST API requests to make predictions / inferences with the model that was trained on the model server. Potentially, we could have many such servers. \n",
    "3. **Client** - client application (browser, mobile app etc.) that needs a prediction, and requests from **inference server** over HTTP with REST API to make the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "In the previous exercise, you had all the 3 servers on your local computer. But that would almost never be the case.  Now let's make it real!!! Your **inference server** will be hosted in the cloud.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General instructions:\n",
    "- No need to touch the training code for this exercise.  The model that was saved in exercise 1 will serve us for this exercise.\n",
    "- You will probably not need to change your **inference server** code and use it as is from exercise 1, but if something doesn't work - then fix it.\n",
    "- You can reuse the **client** code from exercise 1, but now instead of the **inference server** being local, you will need to make an http request to a Heroku server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations\n",
    "1. Create a git repository on Github with your PyCharm project for **inference server**, it's code and saved model from exercise 1\n",
    "1. Create a Heroku account in order to deploy your application\n",
    "1. Create a `Procfile` in your the directory using `gunicorn`. Copy it here for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create `requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `requirements.txt` file that will be used by Heroku to install all of the prerequisites modules for your inference application:\n",
    "1. Create a Python virtual environment on top of bare Python installation (freshly downloaded, not one you installed many additional packages on) - otherwise it will be hard to see what you actually need for the specific project.\n",
    "1. Use the virtual environment only for **inference server** code (otherwise it will include unnecessary modules).\n",
    "1. Create a `requirements.txt` file.  You can do it manually or by using `pip freeze`: https://pip.pypa.io/en/stable/reference/pip_freeze/ \n",
    "1. Did you include the modules version numbers in the `requirements.txt`?  Explain pluses and minuses of including them, and why you chose what you chose\n",
    "1. Add this file to your PyCharm project main direction and Github repo\n",
    "\n",
    "**Remember**:\n",
    "  - Having too few modules will mean inference server that will not work\n",
    "  - Having too many modules means paying for unnecessary storage and maintanence in potentially large number of inference servers.  \n",
    "  \n",
    "Copy your `requirements.txt` here for reference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy your inference application on Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps we covered in the lecture to deploy your application on Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consume you model with python - single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python `requests` module from here to make a single prediction with the inference application hosted on Heroku.\n",
    "- Use the Single prediction api (without JSON file)\n",
    "- Print:\n",
    "  - the URL of the prediction \n",
    "  - all inputs\n",
    "  - output of the prediction\n",
    "\n",
    "**Warning**: don't get used to seeing it in a Jupyter notebook.  This code will usually run inside a **client application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Consume you model with python - multiple predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python `requests` module from here to make multiple predictions with the inference application hosted on Heroku.\n",
    "- Use the Multiple prediction api (with JSON file)\n",
    "- Print:\n",
    "  - the URL of the prediction \n",
    "  - **the input, and the output** of the prediction (or part of it if it's too large).\n",
    "\n",
    "**Warning**: don't get used to seeing it in a Jupyter notebook.  This code will usually run inside a **client application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Submission:\n",
    "1. Paste here your Heroku's URL: \n",
    "1. Paste here your Github repository URL: \n",
    "1. Submit this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bonus: Create a friendly interface (HTML Form) for your single Prediction API\n",
    "Instead of returning your Single Prediction API as string/text, write some HTML form that summarizes the inputs and the prediction.\n",
    "\n",
    "Paste here a picture of calling this API from a browser: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
