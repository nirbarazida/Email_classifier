{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving your model exercise. Part 2 - Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder\n",
    "Reminder, there will usually be 3 different places where the code relevant to our model prediction runs:\n",
    "1. **Training computer / server** - where we train our model and save it\n",
    "2. **Inference server** - server that listens to REST API requests to make predictions / inferences with the model that was trained on the model server. Potentially, we could have many such servers. \n",
    "3. **Client** - client application (browser, mobile app etc.) that needs a prediction, and requests from **inference server** over HTTP with REST API to make the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "In the previous exercise, you had all the 3 servers on your local computer. But that would almost never be the case.  Now let's make it real!!! Your **inference server** will be hosted in the cloud.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General instructions:\n",
    "- No need to touch the training code for this exercise.  The model that was saved in exercise 1 will serve us for this exercise.\n",
    "- You will probably not need to change your **inference server** code and use it as is from exercise 1, but if something doesn't work - then fix it.\n",
    "- You can reuse the **client** code from exercise 1, but now instead of the **inference server** being local, you will need to make an http request to a Heroku server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations\n",
    "1. Create a git repository on Github with your PyCharm project for **inference server**, it's code and saved model from exercise 1\n",
    "1. Create a Heroku account in order to deploy your application\n",
    "1. Create a `Procfile` in your the directory using `gunicorn`. Copy it here for reference:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "git repository: https://github.com/nirbarazida/Email_classifier\n",
    "\n",
    "Procfile:\n",
    "\n",
    "# web: python main.py\n",
    "web: gunicorn main:app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create `requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `requirements.txt` file that will be used by Heroku to install all of the prerequisites modules for your inference application:\n",
    "1. Create a Python virtual environment on top of bare Python installation (freshly downloaded, not one you installed many additional packages on) - otherwise it will be hard to see what you actually need for the specific project.\n",
    "1. Use the virtual environment only for **inference server** code (otherwise it will include unnecessary modules).\n",
    "1. Create a `requirements.txt` file.  You can do it manually or by using `pip freeze`: https://pip.pypa.io/en/stable/reference/pip_freeze/ \n",
    "1. Did you include the modules version numbers in the `requirements.txt`?  Explain pluses and minuses of including them, and why you chose what you chose\n",
    "1. Add this file to your PyCharm project main direction and Github repo\n",
    "\n",
    "**Remember**:\n",
    "  - Having too few modules will mean inference server that will not work\n",
    "  - Having too many modules means paying for unnecessary storage and maintanence in potentially large number of inference servers.  \n",
    "  \n",
    "Copy your `requirements.txt` here for reference: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bcrypt==3.1.7\n",
    "certifi==2020.6.20\n",
    "cffi==1.14.1\n",
    "chardet==3.0.4\n",
    "click==7.1.2\n",
    "dnspython==2.0.0\n",
    "email-validator==1.1.1\n",
    "Flask==1.1.2\n",
    "Flask-Bcrypt==0.7.1\n",
    "Flask-Login==0.5.0\n",
    "Flask-SQLAlchemy==2.4.4\n",
    "Flask-WTF==0.14.3\n",
    "gunicorn==20.0.4\n",
    "idna==2.10\n",
    "itsdangerous==1.1.0\n",
    "Jinja2==2.11.2\n",
    "joblib==0.16.0\n",
    "MarkupSafe==1.1.1\n",
    "numpy==1.19.1\n",
    "pandas==1.1.0\n",
    "pycparser==2.20\n",
    "python-dateutil==2.8.1\n",
    "pytz==2020.1\n",
    "scikit-learn==0.23.2\n",
    "scipy==1.5.2\n",
    "six==1.15.0\n",
    "sklearn==0.0\n",
    "SQLAlchemy==1.3.18\n",
    "threadpoolctl==2.1.0\n",
    "urllib3==1.25.10\n",
    "Werkzeug==1.0.1\n",
    "WTForms==2.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy your inference application on Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps we covered in the lecture to deploy your application on Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consume you model with python - single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python `requests` module from here to make a single prediction with the inference application hosted on Heroku.\n",
    "- Use the Single prediction api (without JSON file)\n",
    "- Print:\n",
    "  - the URL of the prediction \n",
    "  - all inputs\n",
    "  - output of the prediction\n",
    "\n",
    "**Warning**: don't get used to seeing it in a Jupyter notebook.  This code will usually run inside a **client application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = {\"content\":\"This is spam! blah blah blah\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam\n"
     ]
    }
   ],
   "source": [
    "url = 'https://nbemailclassifier.herokuapp.com/classifier'\n",
    "\n",
    "with requests.Session() as s:\n",
    "    pred = s.get(url, params=content).text\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Consume you model with python - multiple predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python `requests` module from here to make multiple predictions with the inference application hosted on Heroku.\n",
    "- Use the Multiple prediction api (with JSON file)\n",
    "- Print:\n",
    "  - the URL of the prediction \n",
    "  - **the input, and the output** of the prediction (or part of it if it's too large).\n",
    "\n",
    "**Warning**: don't get used to seeing it in a Jupyter notebook.  This code will usually run inside a **client application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"email1\":{\"content\":\"\"\"\n",
    "                                It looks like you've deployed your first Linode! Congrats!\n",
    "                                Since you've officially kicked off your journey,\n",
    "                                we wanted to check in and make sure you know about all of the tools and products that are now at your fingertips. Whether you're just here for the compute,\n",
    "                                or you've got plans to build the most sophisticated cloud platform known to humankind, it's important that you know what sort of capabilities are available to \n",
    "                               \"\"\"\n",
    "                   },\n",
    "         \"email2\":{\"content\":\"\"\"\n",
    "                            Next week's lectures:\n",
    "                            Sunday - HMM with Danielle\n",
    "                            -Markov Chains\n",
    "                            -Hidden Markov Models - motivation and applications\n",
    "                            -Dynamic Programming\n",
    "                            -Viterbi, Forward-backward, and Baum-Welch Algorithms\n",
    "                            -Using HMMs in Python\n",
    "\n",
    "                            Monday - CNN with Mike (guest lecturer)\n",
    "                            -CNN architecture overview\n",
    "                            -Convolution\n",
    "                            -Typical CNN layers\n",
    "                            -CNN advantages\n",
    "                            -CNN applications\n",
    "                            Intro to Hackathon and inspiration from Netafim\n",
    "\n",
    "                            Tuesday - Optimizers with Morris\n",
    "                            -Gradient descent - review\n",
    "                            -Stochastic gradient descent (SGD)\n",
    "                            -Mini-batches and batch size\n",
    "                            -Adaptive learning rate methods: Momentum, RMSProp and Adam\n",
    "                            -Bonus: cyclical learning rates\n",
    "                            Hackathon Inspiration Panel - Aleph Farms\n",
    "\n",
    "                            Wednesday - RNN with Morris\n",
    "                            -Motivation\n",
    "                            -Simple RNNs\n",
    "                            -LSTMs and GRUs\n",
    "                            -RNN syntax in Tensorflow\n",
    "\n",
    "                            Thursday - Monitoring NNs and with Morris\n",
    "                            -Saving and loading models: SavedModel and checkpoint formats\n",
    "                            -Monitoring metrics\n",
    "                            -Tensorflow callbacks: EarlyStopping, ModelCheckpoint, and TensorBoard\n",
    "                            -Hyperparameter optimization\n",
    "                            Intro to CV/NLP with Morris\n",
    "                            -Why we teach tracks: deep learning and “hard problems”\n",
    "                            -Examples of NLP tasks\n",
    "                            -Examples of CV tasks\n",
    "                            -How to stay up to date with the start-of-the-art\n",
    "                            Hackathon Inspiration Panel - Tal-Ya Agriculture Solutions\n",
    "                             \"\"\"\n",
    "                 },\n",
    "         \"email3\":{\"content\":\"\"\"\n",
    "                            Hi Nir,\n",
    "\n",
    "                            Today we conducted the 1st checkpoint of the Fellows program.\n",
    "\n",
    "                            The program staff evaluated each of you at the checkpoint according to the following criteria:\n",
    "                            Exam grade\n",
    "                            Saf exercises (% of submissions and the quality of the assignments).\n",
    "                            Project\n",
    "                            Mentor and lecturers feedback\n",
    "                            Attitude and behavior\n",
    "                            General comments\n",
    "                            Congratulations! You officially passed the 1st CP of the program.\n",
    "                            If any critical points came up regarding the discussion about you, to preserve or improve, the program staff will contact you to talk about it. We will try to speak with you all in the upcoming week.\n",
    "                            Keep up the good work!\n",
    "                            Chay and the Fellows staff\n",
    "                             \"\"\"\n",
    "                 }\n",
    "        \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "JSON_FILE_NAME = 'emails_input.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_FILE_NAME, 'w') as outfile:\n",
    "    json.dump(param, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_FILE_NAME, \"r\") as json_file:\n",
    "    emails_to_pred = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"email1\":{\"content\":\"\\n                                It looks like you've deployed your first Linode! Congrats!\\n                                Since you've officially kicked off your journey,\\n                                we wanted to check in and make sure you know about all of the tools and products that are now at your fingertips. Whether you're just here for the compute,\\n                                or you've got plans to build the most sophisticated cloud platform known to humankind, it's important that you know what sort of capabilities are available to \\n                               \",\"label\":\"Spam\"},\"email2\":{\"content\":\"\\n                            Next week's lectures:\\n                            Sunday - HMM with Danielle\\n                            -Markov Chains\\n                            -Hidden Markov Models - motivation and applications\\n                            -Dynamic Programming\\n                            -Viterbi, Forward-backward, and Baum-Welch Algorithms\\n                            -Using HMMs in Python\\n\\n                            Monday - CNN with Mike (guest lecturer)\\n                            -CNN architecture overview\\n                            -Convolution\\n                            -Typical CNN layers\\n                            -CNN advantages\\n                            -CNN applications\\n                            Intro to Hackathon and inspiration from Netafim\\n\\n                            Tuesday - Optimizers with Morris\\n                            -Gradient descent - review\\n                            -Stochastic gradient descent (SGD)\\n                            -Mini-batches and batch size\\n                            -Adaptive learning rate methods: Momentum, RMSProp and Adam\\n                            -Bonus: cyclical learning rates\\n                            Hackathon Inspiration Panel - Aleph Farms\\n\\n                            Wednesday - RNN with Morris\\n                            -Motivation\\n                            -Simple RNNs\\n                            -LSTMs and GRUs\\n                            -RNN syntax in Tensorflow\\n\\n                            Thursday - Monitoring NNs and with Morris\\n                            -Saving and loading models: SavedModel and checkpoint formats\\n                            -Monitoring metrics\\n                            -Tensorflow callbacks: EarlyStopping, ModelCheckpoint, and TensorBoard\\n                            -Hyperparameter optimization\\n                            Intro to CV/NLP with Morris\\n                            -Why we teach tracks: deep learning and \\u201chard problems\\u201d\\n                            -Examples of NLP tasks\\n                            -Examples of CV tasks\\n                            -How to stay up to date with the start-of-the-art\\n                            Hackathon Inspiration Panel - Tal-Ya Agriculture Solutions\\n                             \",\"label\":\"Spam\"},\"email3\":{\"content\":\"\\n                            Hi Nir,\\n\\n                            Today we conducted the 1st checkpoint of the Fellows program.\\n\\n                            The program staff evaluated each of you at the checkpoint according to the following criteria:\\n                            Exam grade\\n                            Saf exercises (% of submissions and the quality of the assignments).\\n                            Project\\n                            Mentor and lecturers feedback\\n                            Attitude and behavior\\n                            General comments\\n                            Congratulations! You officially passed the 1st CP of the program.\\n                            If any critical points came up regarding the discussion about you, to preserve or improve, the program staff will contact you to talk about it. We will try to speak with you all in the upcoming week.\\n                            Keep up the good work!\\n                            Chay and the Fellows staff\\n                             \",\"label\":\"Spam\"},\"label\":[\"Spam\",\"Spam\",\"Spam\"]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as s:\n",
    "    pred = s.get(url, json=emails_to_pred)\n",
    "    print(pred.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Submission:\n",
    "1. Paste here your Heroku's URL: \n",
    "1. Paste here your Github repository URL: \n",
    "1. Submit this notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Heroku's URL: https://nbemailclassifier.herokuapp.com/home\n",
    "\n",
    "Github repository URL: https://github.com/nirbarazida/Email_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bonus: Create a friendly interface (HTML Form) for your single Prediction API\n",
    "Instead of returning your Single Prediction API as string/text, write some HTML form that summarizes the inputs and the prediction.\n",
    "\n",
    "Paste here a picture of calling this API from a browser: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
