{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving your model exercise. Part 1 - Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**created by:** Nir Barazida\n",
    "\n",
    "**project:** email classifier application deployed on on-line server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes to the checker !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear checker,\n",
    "\n",
    "In this exercise, I have implemented a flask app with user interface.\n",
    "\n",
    "features:\n",
    "\n",
    "- User registration to the application (saved in SQLite database)\n",
    "- Classifier for emails - spam or ham, pickled and deployed. only users can use  the classifier (canceled this feature to use request - can't connect as a user using requests - opened a help request on hive)\n",
    "- stores all the classified email per user in the database and show it on the classifier page.\n",
    "- Classifier using param or json file.\n",
    "- deployed the app on a [remote server](http://172.104.139.73/) - having troubles with Nginx - opened a help request on hive.\n",
    "- To run the app - please run main and please go to your localhost at port 5000 to see the user interface.\n",
    "- created a [git repository](https://github.com/nirbarazida/Email_classifier) with the project.\n",
    "\n",
    "It's the first time using this technology (flask and deploy the app on server) - would very appreciate getting notes from you about **everything** - project architecture, functions, coding, readme - **every word** will be appreciated \n",
    "\n",
    "### p.s.\n",
    "Used my own python packege in this Exercise - just `pip install NBprocessing` to use it\n",
    "\n",
    "Thanks,\n",
    "\n",
    "Nir Barazida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install NBprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "Reminder, there will usually be 3 different places where the code relevant to our model prediction runs:\n",
    "1. **Training computer / server** - where we train our model and save it\n",
    "2. **Inference server** - server that listens to REST API requests to make predictions / inferences with the model that was trained on the model server. Potentially, we could have many such servers. \n",
    "3. **Client** - client application (browser, mobile app etc.) that needs a prediction, and requests from **inference server** over HTTP with REST API to make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from NBprocessing import NBgeneral, NBplot, NBcategorical\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting to a trained model\n",
    "- Choose one of the models you trained in one the previous exercises or any other model. **Do not take something from many Flask examples online!**\n",
    "- Specify where can one download the dataset from (to be used during checking the exercise)\n",
    "- Say in one word what is the business problem and what you are predicting \n",
    "- Preprocess, split to train and test dataset\n",
    "- Train the model - the exact accuracy is not of big importance here\n",
    "- Do a few predictions of the model locally\n",
    "\n",
    "This code will run on the **training server**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset:\n",
    "\n",
    "In this exercise I will use the enron dataset that can be found on our [Google drive](https://drive.google.com/drive/folders/1mK-Au2ScIxG1_OMfKfY3Q7VzgkAf-uWx).\\\n",
    "I will use this data to train a spam filter, using a processed version of the Enron dataset including labels for \"ham\" (non-spam) and spam emails, provided by the [NLP group at the Athens University of Economics and Business](http://nlp.cs.aueb.gr/software.html) (AUEB).\\\n",
    "I this case I will use the AUEB predictions as the true label of the data and will try to classify the data for ham or spam myself.\\\n",
    "I will also zip the csv file to this solution.\n",
    "\n",
    "#### business problem:\n",
    "\n",
    "Classify emails as ham or sapm\n",
    "\n",
    "#### Preprocess, split to train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv('enron.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38891, 3)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', 'label', 'text'], dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26844</th>\n",
       "      <td>enron5/spam/1042.2002-08-25.SA_and_HP.spam.txt</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: better sex for $ 2 . 99 until 08 / 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145</th>\n",
       "      <td>enron6/ham/4984.2001-11-14.lokay.ham.txt</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: fw :\\nanother mention\\nthen , at an a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename label  \\\n",
       "26844  enron5/spam/1042.2002-08-25.SA_and_HP.spam.txt  spam   \n",
       "4145         enron6/ham/4984.2001-11-14.lokay.ham.txt   ham   \n",
       "\n",
       "                                                    text  \n",
       "26844  Subject: better sex for $ 2 . 99 until 08 / 26...  \n",
       "4145   Subject: fw :\\nanother mention\\nthen , at an a...  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enron1/ham/1061.2000-05-10.farmer.ham.txt</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: ena sales on hpl\\njust to update you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enron1/ham/0446.2000-02-18.farmer.ham.txt</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: 98 - 6736 &amp; 98 - 9638 for 1997 ( ua 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename label  \\\n",
       "0  enron1/ham/1061.2000-05-10.farmer.ham.txt   ham   \n",
       "1  enron1/ham/0446.2000-02-18.farmer.ham.txt   ham   \n",
       "\n",
       "                                                text  \n",
       "0  Subject: ena sales on hpl\\njust to update you ...  \n",
       "1  Subject: 98 - 6736 & 98 - 9638 for 1997 ( ua 4...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['label']=='ham'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18045</th>\n",
       "      <td>enron1/spam/4743.2005-06-25.GP.spam.txt</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: what up , , your cam babe\\nwhat are y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18046</th>\n",
       "      <td>enron1/spam/1309.2004-06-08.GP.spam.txt</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: want to make more money ?\\norder conf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      filename label  \\\n",
       "18045  enron1/spam/4743.2005-06-25.GP.spam.txt  spam   \n",
       "18046  enron1/spam/1309.2004-06-08.GP.spam.txt  spam   \n",
       "\n",
       "                                                    text  \n",
       "18045  Subject: what up , , your cam babe\\nwhat are y...  \n",
       "18046  Subject: want to make more money ?\\norder conf...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['label']=='spam'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_5ddbc1b4_dd5d_11ea_92e1_34e6ad127590row0_col0 {\n",
       "            color:  black;\n",
       "        }    #T_5ddbc1b4_dd5d_11ea_92e1_34e6ad127590row0_col1 {\n",
       "            color:  black;\n",
       "        }</style><table id=\"T_5ddbc1b4_dd5d_11ea_92e1_34e6ad127590\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_5ddbc1b4_dd5d_11ea_92e1_34e6ad127590level0_row0\" class=\"row_heading level0 row0\" >label</th>\n",
       "                        <td id=\"T_5ddbc1b4_dd5d_11ea_92e1_34e6ad127590row0_col0\" class=\"data row0 col0\" >spam : 53.6%</td>\n",
       "                        <td id=\"T_5ddbc1b4_dd5d_11ea_92e1_34e6ad127590row0_col1\" class=\"data row0 col1\" >ham : 46.4%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20bebddab08>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBcategorical.category_ratio(data,['label'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: ena sales on hpl\\njust to update you on this project ' s status :\\nbased on a new report that scott mills ran for me from sitara , i have come up\\nwith the following counterparties as the ones to which ena is selling gas off\\nof hpl ' s pipe .\\naltrade transaction , l . l . c . gulf gas utilities company\\nbrazoria , city of panther pipeline , inc .\\ncentral illinois light company praxair , inc .\\ncentral power and light company reliant energy - entex\\nces - equistar chemicals , lp reliant energy - hl & p\\ncorpus christi gas marketing , lp southern union company\\nd & h gas company , inc . texas utilities fuel company\\nduke energy field services , inc . txu gas distribution\\nentex gas marketing company union carbide corporation\\nequistar chemicals , lp unit gas transmission company inc .\\nsince i ' m not sure exactly what gets entered into sitara , pat clynes\\nsuggested that i check with daren farmer to make sure that i ' m not missing\\nsomething ( which i did below ) . while i am waiting for a response from him\\nand / or mary smith , i will begin gathering the contractual volumes under the\\nabove contracts .\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by cheryl dudley / hou / ect on 05 / 10 / 2000 07 : 56\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\ncheryl d king\\n05 / 08 / 2000 04 : 11 pm\\nsent by : cheryl dudley\\nto : daren j farmer / hou / ect @ ect , mary m smith / hou / ect @ ect\\ncc :\\nsubject : ena sales on hpl\\ni am working on a project for brenda herod & was wondering if one of you\\ncould tell me if i ' m on the right track & if this will get everything for\\nwhich she is looking .\\nshe is trying to draft a long - term transport / storage agreement between ena &\\nhplc which will allow ena to move the gas to their markets . in order to\\naccomplish this , she needs to know all of the sales to customers that ena is\\ndoing off of hpl ' s pipe .\\ni had scott mills run a report from sitara showing all ena buy / sell activity\\non hpl since 7 / 99 . if i eliminate the buys & the desk - to - desk deals , will\\nthis give me everything that i need ?\\nare there buy / sell deals done with ena on hpl ' s pipe that wouldn ' t show up in\\nsitara ? someone mentioned something about deals where hpl transports the gas\\non it ' s own behalf then ena sells it to a customer at that same spot - -\\n? ? ? ? ? do deals like that happen ? would they show up in sitara ?\\nis there anything else that i ' m missing ? i ' m not real familiar with how some\\nof these deals happen nowadays so am very receptive to any\\nideas / suggestions / help that you can offer ! ! !\\nthanks in advance .\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NBplot.plot_missing_value_heatmap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Missing_values</th>\n",
       "      <th>%Missing_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [#Missing_values, %Missing_values]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBgeneral.missing_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "\n",
    "- We can tell that the data is balanced - almost split into half for ham and sapm.\n",
    "- ~39K of indexes is a far share of data - can create a reliable model\n",
    "- we have no missing values in the dataset\n",
    "- The text has:\n",
    "    - a lot of punctuation and '\\n'\n",
    "    - lower and upper case letters\n",
    "    - unexpected numbers and letters - probably bad encoding of the text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20846\n",
       "0    18045\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the label to numeric\n",
    "y = data['label'].map({'ham':0, 'spam':1})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38891,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text of emails to a feature matrix X whose rows are \"bag of words\" feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first 'claen' the text indexes\n",
    "clean_text = data['text'].map(lambda x: x.lower().replace('\\n', '').translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '2000', '2001', '2002', '2004', '2005', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '40', '45', '50', '500', '60', '70', '713', '75', '80', '90', '95', '97', '99', 'able', 'access', 'account', 'act', 'action', 'additional', 'address', 'adobe', 'advice', 'agreement', 'alias', 'america', 'amto', 'analyst', 'approved', 'april', 'assets', 'attached', 'august', 'available', 'bank', 'based', 'believe', 'best', 'better', 'big', 'billion', 'board', 'book', 'business', 'buy', 'california', 'capacity', 'capital', 'case', 'cash', 'cc', 'center', 'ceo', 'chairman', 'change', 'changes', 'check', 'chief', 'click', 'close', 'com', 'come']\n"
     ]
    }
   ],
   "source": [
    "# create 500 vectores of words. each vector is a combination of 1-2 words\n",
    "email_text_list = clean_text.tolist()\n",
    "vectorizer = CountVectorizer( encoding='utf-8', decode_error='ignore',stop_words = 'english',\n",
    "                             analyzer='word', ngram_range=(1, 2),max_features=500)\n",
    "X = vectorizer.fit_transform(email_text_list)\n",
    "print(vectorizer.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38891, 500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>weeks</th>\n",
       "      <th>wish</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31799</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  01  02  03  04  05  06  07  08  ...  weeks  wish  work  \\\n",
       "30360   0    0   0   0   0   0   0   0   0   0  ...      0     0     0   \n",
       "31799   0    0   0   0   0   0   0   0   0   0  ...      0     0     0   \n",
       "\n",
       "       working  works  world  www  year  years  york  \n",
       "30360        0      0      0    0     0      0     0  \n",
       "31799        0      0      0    0     0      0     0  \n",
       "\n",
       "[2 rows x 500 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert X (Count Vectorizer object) to a data frame with labels name\n",
    "X = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "X.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each row represent how many times the vector appears in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (38891, 500)\n",
      "y shape: (38891,)\n"
     ]
    }
   ],
   "source": [
    "# check datasets dimensions\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \\\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will  use a random  forest calssifier and to correctly choose the Hyper parameters I will use the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) tool - Exhaustive search over specified parameter values for an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.ensemble.RandomForestClassifier(`n_estimators=100`, *, criterion='gini', `max_depth=None`, `min_samples_split=2`, `min_samples_leaf=1`, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None\n",
    "\n",
    "`n_estimators` - The number of trees in the forest.\\\n",
    "`max_depth` - The maximum depth of the tree.\\\n",
    "`min_samples_split` - The minimum number of samples required to split an internal node.\\\n",
    "`min_samples_leaf` - The minimum number of samples required to be at a leaf node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [100, 150, 200],\n",
       " 'max_depth': [10, 20],\n",
       " 'min_samples_split': [1, 2],\n",
       " 'min_samples_leaf': [2, 4]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth = [10,20]\n",
    "min_samples_split = [1,2]\n",
    "min_samples_leaf = [2,4]\n",
    "n_estimators = [100,150,200]\n",
    "param_grid_rf = {'n_estimators':n_estimators,\n",
    "              'max_depth':max_depth,\n",
    "             'min_samples_split':min_samples_split,\n",
    "             'min_samples_leaf':min_samples_leaf}\n",
    "param_grid_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_grid_rf = GridSearchCV(RandomForestClassifier(),\n",
    "                             param_grid=param_grid_rf,\n",
    "                             cv=3, verbose=2, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=3,\n",
       "             param_grid={'max_depth': [10, 20], 'min_samples_leaf': [2, 4],\n",
       "                         'min_samples_split': [1, 2],\n",
       "                         'n_estimators': [100, 150, 200]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 20,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=20, min_samples_leaf=2, n_estimators=150)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(**clf_grid_rf.best_params_)\n",
    "clf_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Random Forest Classifier:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93      3590\n",
      "           1       0.90      0.99      0.94      4189\n",
      "\n",
      "    accuracy                           0.94      7779\n",
      "   macro avg       0.95      0.93      0.94      7779\n",
      "weighted avg       0.94      0.94      0.94      7779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classification report for Random Forest Classifier:\\n\\n\")\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model score is not that good, let's try with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [100, 150, 200]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [100,150,200]\n",
    "param_grid_rf = {'n_estimators':n_estimators}\n",
    "param_grid_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   9 out of   9 | elapsed:  4.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   9 out of   9 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=3,\n",
       "             param_grid={'n_estimators': [100, 150, 200]}, verbose=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid_rf_2 = GridSearchCV(RandomForestClassifier(),\n",
    "                             param_grid=param_grid_rf,\n",
    "                             cv=3, verbose=2, n_jobs=3)\n",
    "clf_grid_rf_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 150}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid_rf_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=150)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_2 = RandomForestClassifier(**clf_grid_rf_2.best_params_)\n",
    "clf_rf_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = clf_rf_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Random Forest Classifier:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      3590\n",
      "           1       0.97      0.99      0.98      4189\n",
      "\n",
      "    accuracy                           0.97      7779\n",
      "   macro avg       0.97      0.97      0.97      7779\n",
      "weighted avg       0.97      0.97      0.97      7779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classification report for Random Forest Classifier:\\n\\n\")\n",
    "print(classification_report(y_test,y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this business problem we are predicting if a email is sapm or not.\\\n",
    "As a company that provides email services we would like to make sure that **all** real emails will go to the in-box, even in case of sapm leakage to the the in-box\\\n",
    "Thus, we wolud like to max the recall (min the FN).\\\n",
    "From the classification report we can tell that the recall score is very high - the model works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save you model, predict with saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate in this notebook code that will happen during training on the **training server**:\n",
    "- Using `pickle`, save your model to disk. Reference: https://scikit-learn.org/stable/modules/model_persistence.html\n",
    "- Save the test dataset to file.  What's a good format(s) for saving datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vectorizer_model_for_SOH.pkl'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'rfc_model_for_SOH.pkl'\n",
    "pickle.dump(clf_rf_2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Random Forest Classifier from pickle file:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      3590\n",
      "           1       0.97      0.99      0.98      4189\n",
      "\n",
      "    accuracy                           0.97      7779\n",
      "   macro avg       0.97      0.97      0.97      7779\n",
      "weighted avg       0.97      0.97      0.97      7779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check that it worked\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "y_pred_2 = loaded_model.predict(X_test)\n",
    "print(f\"Classification report for Random Forest Classifier from pickle file:\\n\\n\")\n",
    "print(classification_report(y_test,y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### great !\n",
    "We can see that we've got the same results as the original model (some minor changes, because of the score rounding) .\\\n",
    "What happened is that `pickle` encoded our model and saved it as a pickle object. Now the model is saved on my local disk and can be used, after unpacking it, again and not create, parametrize and train another model.\\\n",
    "wow - that can save some time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate in this notebook code that will happen during inference on the **inference server**:\n",
    "- Load the model again with `pickle`.\n",
    "- Read the test dataset file, and perform some predictions\n",
    "- Compare the predictions received before saving the model, and after reading a saved model.  Show that you get the same results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answered it in the above (:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Serve your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with **training server**, since we have the saved model.  From now all that's relevant is **inference server** and **client** code.\n",
    "\n",
    "Let's create the **inference server** that answers to REST APIs with predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `flask`, create a Pycharm project and implement the following prediction API:\n",
    "- **Single prediction API** that receives inputs as parameters (no body), and returns a single prediction as a string / text.  \n",
    "- Example: http://localhost:5000/predict_single?key1=value1&key2=value2 (replace `key1`, `value1` etc. names with your relevant feature names and values)\n",
    "- Consider what's the best place in your code to put the code that reads from the model.  Why?\n",
    "- In general, take runtime efficiency into account.  Your API might be called large number of times per second, and you will be paying for more inference servers if your code is not efficient.\n",
    "- Copy your **inference server** code also here for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It doesn't work. can't connect to the API as user - need your help here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <!-- Required meta tags -->\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n",
      "\n",
      "    <!-- Bootstrap CSS -->\n",
      "    <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css\" integrity=\"sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm\" crossorigin=\"anonymous\">\n",
      "\n",
      "    <link rel=\"stylesheet\" type=\"text/css\" href=\"/static/main.css\">\n",
      "\n",
      "    <!-- Name on the toolbar-->\n",
      "    \n",
      "        <title>Spam or Ham</title>\n",
      "    \n",
      "</head>\n",
      "<body>\n",
      "    <header class=\"site-header\">\n",
      "      <nav class=\"navbar navbar-expand-md navbar-dark bg-steel fixed-top\">\n",
      "        <div class=\"container\">\n",
      "          <a class=\"navbar-brand mr-4\" href=\"/home\">Spam or Ham</a>\n",
      "          <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarToggle\" aria-controls=\"navbarToggle\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\n",
      "            <span class=\"navbar-toggler-icon\"></span>\n",
      "          </button>\n",
      "          <div class=\"collapse navbar-collapse\" id=\"navbarToggle\">\n",
      "            <div class=\"navbar-nav mr-auto\">\n",
      "              <a class=\"nav-item nav-link\" href=\"/home\">Home</a>\n",
      "              <a class=\"nav-item nav-link\" href=\"/classifier\">Classifier</a>\n",
      "            </div>\n",
      "            <!-- Navbar Right Side -->\n",
      "            <div class=\"navbar-nav\">\n",
      "                \n",
      "                  <a class=\"nav-item nav-link\" href=\"/login\">Login</a>\n",
      "                  <a class=\"nav-item nav-link\" href=\"/register\">Register</a>\n",
      "                \n",
      "            </div>\n",
      "          </div>\n",
      "        </div>\n",
      "      </nav>\n",
      "    </header>\n",
      "    <main role=\"main\" class=\"container\">\n",
      "      <div class=\"row\">\n",
      "        <div class=\"col-md-8\">\n",
      "            <!-- add alert messages - like - account created. with_categories - success/ warning etc.  -->\n",
      "            \n",
      "                \n",
      "              \n",
      "          \n",
      "<h1>Welcome To The Email Classifier</h1>\n",
      "<div class=\"content-section\">\n",
      "    <p>This Classifier was created as part of a home assignment at the Israeli Tech Challenge Bootcamp.\n",
      "        The main purpose of this classifier is to determine if an email is spam or ham.\n",
      "        </p>\n",
      "        <p>\n",
      "        The model predictions are based on the 'Enron' database provided by the NLP group at the Athens University of Economics and Business <a href=\"http://nlp.cs.aueb.gr/software.html\">(AUEB)</a>.\n",
      "        I've use this data to train a spam filter, using a processed version of the Enron dataset including labels for \"ham\" (non-spam) and spam emails.\n",
      "        I this case I've use the AUEB predictions as the true label of the data and classified the data for ham or spam myself.\n",
      "    </p>\n",
      "    <P>\n",
      "        First I've used 'CountVectorizer' from 'Sklearn' to create Vectorize the words in the dataset into 500 different features that were created from 1-2 words.\n",
      "        After trying different prediction models the one how to produce the best score with 97% of precision is 'Random Forest Classifier'.\n",
      "        To prefect the classifier I have used GridSearchCV from 'Sklearn' to find the best parameters on the train dataset.\n",
      "    </P>\n",
      "    <P>\n",
      "        Then, to deploy them to this website I have used 'Pickle' package to 'zip' them. When the website is activated the models are loaded and can be used to create prediction in last than 1 sec!\n",
      "    </P>\n",
      "    <P>\n",
      "        Hope you enjoy my website and wish you good luck,\n",
      "    </P>\n",
      "\n",
      "    <p>\n",
      "        yours,\n",
      "    </p>\n",
      "    <p>\n",
      "        Nir Barazida\n",
      "    </p>\n",
      "</div>\n",
      "\n",
      "        </div>\n",
      "        <div class=\"col-md-4\">\n",
      "          <div>\n",
      "              \n",
      "                \n",
      "    \n",
      "    <!--content-section - location  on the top right-->\n",
      "        <div class=\"content-section\">\n",
      "            <form method=\"POST\" action=\"\">\n",
      "                <input id=\"csrf_token\" name=\"csrf_token\" type=\"hidden\" value=\"IjE0YTFjY2I5YzJjODk2MWNjNmE3NDhjN2Y5MWE0Nzc1ZjNiMjc5MDUi.XzU4Nw.-N3NoVPc5cQL2jgBLthwQmcuKEM\">\n",
      "                <fieldset class=\"form-group\">\n",
      "                    <div class=\"form-group\">\n",
      "                        <label class=\"form-control-label\" for=\"username\">Username</label>\n",
      "                        \n",
      "                            <input class=\"form-control form-control-lg is-invalid\" id=\"username\" name=\"username\" required type=\"text\" value=\"\">\n",
      "                            <div class=\"invalid-feedback\">\n",
      "                                \n",
      "                                    <span>This field is required.</span>\n",
      "                                \n",
      "                            </div>\n",
      "                        \n",
      "                    </div>\n",
      "\n",
      "                    <div class=\"form-group\">\n",
      "                        <label class=\"form-control-label\" for=\"password\">Password</label>\n",
      "                        \n",
      "                            <input class=\"form-control form-control-lg is-invalid\" id=\"password\" name=\"password\" required type=\"password\" value=\"\">\n",
      "                            <div class=\"invalid-feedback\">\n",
      "                                \n",
      "                                    <span>This field is required.</span>\n",
      "                                \n",
      "                            </div>\n",
      "                        \n",
      "                    </div>\n",
      "                <div class=\"form-check\">\n",
      "                    <input class=\"form-check-input\" id=\"remember\" name=\"remember\" type=\"checkbox\" value=\"y\">\n",
      "                    <label class=\"form-check-label\" for=\"remember\">Remember me</label>\n",
      "                </div>\n",
      "                </fieldset>\n",
      "                <div class=\"form-group\">\n",
      "                    <input class=\"btn btn-outline-info\" id=\"submit\" name=\"submit\" type=\"submit\" value=\"Login\">\n",
      "                </div>\n",
      "                <small class=\"text-muted ml-2\">\n",
      "                    <a href=\"#\">Forgot Password?</a>\n",
      "                </small>\n",
      "            </form>\n",
      "        </div>\n",
      "        <div class=\"border-top pt-3\">\n",
      "            <small class=\"text-muted\">\n",
      "                Need An Account? <a class=\"ml-2\" href=\"/register\">Sign Up Now</a>\n",
      "            </small>\n",
      "        </div>\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "              \n",
      "          </div>\n",
      "        </div>\n",
      "      </div>\n",
      "    </main>\n",
      "\n",
      "\n",
      "    <!-- Optional JavaScript -->\n",
      "    <!-- jQuery first, then Popper.js, then Bootstrap JS -->\n",
      "    <script src=\"https://code.jquery.com/jquery-3.2.1.slim.min.js\" integrity=\"sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN\" crossorigin=\"anonymous\"></script>\n",
      "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js\" integrity=\"sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q\" crossorigin=\"anonymous\"></script>\n",
      "    <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js\" integrity=\"sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl\" crossorigin=\"anonymous\"></script>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\n",
    "    'username': 'admin',\n",
    "    'password': '123456'\n",
    "}\n",
    "\n",
    "with requests.Session() as s:\n",
    "    p = s.post('http://127.0.0.1:5000', params=payload)\n",
    "    # print the html returned or something more intelligent to see if it's a successful login page.\n",
    "    print (p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = {\"content\":\"This is spam! blah blah blah\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as s:\n",
    "    pred = s.get('http://127.0.0.1:5000/classifier', params=content).text\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consume your model with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python `requests` module from here to make a prediction.  **Print input and output of the prediction.**\n",
    "\n",
    "**Warning**: don't get used to seeing it in a Jupyter notebook.  This code will usually run inside a **client application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "did it in the above question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `flask`, add code to your previous file in Pycharm **with inference code** to create the following prediction API (in addition to **Single prediction API** done above):\n",
    "- **Multiple prediction API** that receives input many observations to predict on as a json file in the body, and returns a json file with predictions.\n",
    "- Your **JSON** file format has to be efficient, clear and following JSON file syntax: \n",
    "  - JSON file is a nested structure of potentially multiple dictionaries and lists \n",
    "  - JSON file tip: Use lists, every member in the list can be a dictionary of all the features.  \n",
    "  - JSON file tip: Do not put indexes of predictions into the JSON files, indexes of predictions can be easily computed with Python code later \n",
    "  - JSON files are sometimes slightly verbose, but are extremely human readable.  Just looking at your JSON files of input and output, is it possible to understand what were the observations in input and what were the predictions in output?\n",
    "  - See https://www.json.org/json-en.html for JSON format\n",
    "- Think about efficiency of your code - your REST API might be called a huge number of times, with a huge number of observations every time.  Can part of the code be done only once?  Can you predict on everything together? Can you do less or cheaper data conversions?\n",
    "- Example of URL that will be used to predict: http://localhost:5000/predict\n",
    "- Reference for working with JSONs in Flask: https://pythonise.com/series/learning-flask/working-with-json-in-flask\n",
    "- Do you need a GET or a POST type of REST API call? Does it change what you did in step 3?  Conceptually, would you say it makes sense to use GET or POST types for predictions?\n",
    "- Copy your **inference server** code also here for reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"email1\":{\"content\":\"\"\"\n",
    "                                It looks like you've deployed your first Linode! Congrats!\n",
    "                                Since you've officially kicked off your journey,\n",
    "                                we wanted to check in and make sure you know about all of the tools and products that are now at your fingertips. Whether you're just here for the compute,\n",
    "                                or you've got plans to build the most sophisticated cloud platform known to humankind, it's important that you know what sort of capabilities are available to \n",
    "                               \"\"\"\n",
    "                   },\n",
    "         \"email2\":{\"content\":\"\"\"\n",
    "                            Next week's lectures:\n",
    "                            Sunday - HMM with Danielle\n",
    "                            -Markov Chains\n",
    "                            -Hidden Markov Models - motivation and applications\n",
    "                            -Dynamic Programming\n",
    "                            -Viterbi, Forward-backward, and Baum-Welch Algorithms\n",
    "                            -Using HMMs in Python\n",
    "\n",
    "                            Monday - CNN with Mike (guest lecturer)\n",
    "                            -CNN architecture overview\n",
    "                            -Convolution\n",
    "                            -Typical CNN layers\n",
    "                            -CNN advantages\n",
    "                            -CNN applications\n",
    "                            Intro to Hackathon and inspiration from Netafim\n",
    "\n",
    "                            Tuesday - Optimizers with Morris\n",
    "                            -Gradient descent - review\n",
    "                            -Stochastic gradient descent (SGD)\n",
    "                            -Mini-batches and batch size\n",
    "                            -Adaptive learning rate methods: Momentum, RMSProp and Adam\n",
    "                            -Bonus: cyclical learning rates\n",
    "                            Hackathon Inspiration Panel - Aleph Farms\n",
    "\n",
    "                            Wednesday - RNN with Morris\n",
    "                            -Motivation\n",
    "                            -Simple RNNs\n",
    "                            -LSTMs and GRUs\n",
    "                            -RNN syntax in Tensorflow\n",
    "\n",
    "                            Thursday - Monitoring NNs and with Morris\n",
    "                            -Saving and loading models: SavedModel and checkpoint formats\n",
    "                            -Monitoring metrics\n",
    "                            -Tensorflow callbacks: EarlyStopping, ModelCheckpoint, and TensorBoard\n",
    "                            -Hyperparameter optimization\n",
    "                            Intro to CV/NLP with Morris\n",
    "                            -Why we teach tracks: deep learning and â€œhard problemsâ€\n",
    "                            -Examples of NLP tasks\n",
    "                            -Examples of CV tasks\n",
    "                            -How to stay up to date with the start-of-the-art\n",
    "                            Hackathon Inspiration Panel - Tal-Ya Agriculture Solutions\n",
    "                             \"\"\"\n",
    "                 },\n",
    "         \"email3\":{\"content\":\"\"\"\n",
    "                            Hi Nir,\n",
    "\n",
    "                            Today we conducted the 1st checkpoint of the Fellows program.\n",
    "\n",
    "                            The program staff evaluated each of you at the checkpoint according to the following criteria:\n",
    "                            Exam grade\n",
    "                            Saf exercises (% of submissions and the quality of the assignments).\n",
    "                            Project\n",
    "                            Mentor and lecturers feedback\n",
    "                            Attitude and behavior\n",
    "                            General comments\n",
    "                            Congratulations! You officially passed the 1st CP of the program.\n",
    "                            If any critical points came up regarding the discussion about you, to preserve or improve, the program staff will contact you to talk about it. We will try to speak with you all in the upcoming week.\n",
    "                            Keep up the good work!\n",
    "                            Chay and the Fellows staff\n",
    "                             \"\"\"\n",
    "                 }\n",
    "        \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "JSON_FILE_NAME = 'emails_input.json'\n",
    "\n",
    "with open(JSON_FILE_NAME, 'w') as outfile:\n",
    "    json.dump(param, outfile, indent=2)\n",
    "    \n",
    "with open(JSON_FILE_NAME, \"r\") as json_file:\n",
    "    emails_to_pred = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"email1\": {\n",
      "    \"content\": \"\\n                                It looks like you've deployed your first Linode! Congrats!\\n                                Since you've officially kicked off your journey,\\n                                we wanted to check in and make sure you know about all of the tools and products that are now at your fingertips. Whether you're just here for the compute,\\n                                or you've got plans to build the most sophisticated cloud platform known to humankind, it's important that you know what sort of capabilities are available to \\n                               \", \n",
      "    \"label\": \"Spam\"\n",
      "  }, \n",
      "  \"email2\": {\n",
      "    \"content\": \"\\n                            Next week's lectures:\\n                            Sunday - HMM with Danielle\\n                            -Markov Chains\\n                            -Hidden Markov Models - motivation and applications\\n                            -Dynamic Programming\\n                            -Viterbi, Forward-backward, and Baum-Welch Algorithms\\n                            -Using HMMs in Python\\n\\n                            Monday - CNN with Mike (guest lecturer)\\n                            -CNN architecture overview\\n                            -Convolution\\n                            -Typical CNN layers\\n                            -CNN advantages\\n                            -CNN applications\\n                            Intro to Hackathon and inspiration from Netafim\\n\\n                            Tuesday - Optimizers with Morris\\n                            -Gradient descent - review\\n                            -Stochastic gradient descent (SGD)\\n                            -Mini-batches and batch size\\n                            -Adaptive learning rate methods: Momentum, RMSProp and Adam\\n                            -Bonus: cyclical learning rates\\n                            Hackathon Inspiration Panel - Aleph Farms\\n\\n                            Wednesday - RNN with Morris\\n                            -Motivation\\n                            -Simple RNNs\\n                            -LSTMs and GRUs\\n                            -RNN syntax in Tensorflow\\n\\n                            Thursday - Monitoring NNs and with Morris\\n                            -Saving and loading models: SavedModel and checkpoint formats\\n                            -Monitoring metrics\\n                            -Tensorflow callbacks: EarlyStopping, ModelCheckpoint, and TensorBoard\\n                            -Hyperparameter optimization\\n                            Intro to CV/NLP with Morris\\n                            -Why we teach tracks: deep learning and \\u201chard problems\\u201d\\n                            -Examples of NLP tasks\\n                            -Examples of CV tasks\\n                            -How to stay up to date with the start-of-the-art\\n                            Hackathon Inspiration Panel - Tal-Ya Agriculture Solutions\\n                             \", \n",
      "    \"label\": \"Ham\"\n",
      "  }, \n",
      "  \"email3\": {\n",
      "    \"content\": \"\\n                            Hi Nir,\\n\\n                            Today we conducted the 1st checkpoint of the Fellows program.\\n\\n                            The program staff evaluated each of you at the checkpoint according to the following criteria:\\n                            Exam grade\\n                            Saf exercises (% of submissions and the quality of the assignments).\\n                            Project\\n                            Mentor and lecturers feedback\\n                            Attitude and behavior\\n                            General comments\\n                            Congratulations! You officially passed the 1st CP of the program.\\n                            If any critical points came up regarding the discussion about you, to preserve or improve, the program staff will contact you to talk about it. We will try to speak with you all in the upcoming week.\\n                            Keep up the good work!\\n                            Chay and the Fellows staff\\n                             \", \n",
      "    \"label\": \"Ham\"\n",
      "  }, \n",
      "  \"label\": [\n",
      "    \"Spam\", \n",
      "    \"Ham\", \n",
      "    \"Ham\"\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as s:\n",
    "    pred = s.get('http://127.0.0.1:5000/classifier', json=emails_to_pred)\n",
    "    print(pred.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python `requests` module from here to make a prediction, and **print the input, and the output** of the prediction (or part of it if it's too large).  \n",
    "\n",
    "**Warning**: don't get used to seeing it in a Jupyter notebook.  This code will usually run inside a **client application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "did it in the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Submit a zip file with:\n",
    "1. This notebook\n",
    "2. Your Python inference server file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
